{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmPGe7C5PW3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320d0030-a8f2-40d9-8c3c-28e54a50bbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen\n",
            "  Downloading pyautogen-0.8.3-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyautogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from pyautogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from pyautogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from pyautogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyautogen) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen) (2.10.6)\n",
            "Collecting python-dotenv (from pyautogen)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyautogen) (2.5.0)\n",
            "Collecting tiktoken (from pyautogen)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->pyautogen) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen) (2.27.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->pyautogen) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->pyautogen) (3.4.1)\n",
            "Downloading pyautogen-0.8.3-py3-none-any.whl (685 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m685.4/685.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, diskcache, tiktoken, docker, asyncer, pyautogen\n",
            "Successfully installed asyncer-0.0.8 diskcache-5.6.3 docker-7.1.0 pyautogen-0.8.3 python-dotenv-1.1.0 tiktoken-0.9.0\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=45cedcd69f9b440f499cfdb4624671d8070356c12c4494eabdb9746f48557d50\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n",
            "Collecting scholarly\n",
            "  Downloading scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting arrow (from scholarly)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from scholarly) (4.13.3)\n",
            "Collecting bibtexparser (from scholarly)\n",
            "  Downloading bibtexparser-1.4.3.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from scholarly) (1.2.18)\n",
            "Collecting fake-useragent (from scholarly)\n",
            "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting free-proxy (from scholarly)\n",
            "  Downloading free_proxy-1.1.3.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from scholarly) (0.28.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from scholarly) (1.1.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from scholarly) (2.32.3)\n",
            "Collecting selenium (from scholarly)\n",
            "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting sphinx-rtd-theme (from scholarly)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scholarly) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->scholarly) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->scholarly)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->scholarly) (2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from bibtexparser->scholarly) (3.2.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->scholarly) (1.17.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from free-proxy->scholarly) (5.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->scholarly) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->scholarly) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->scholarly) (2.3.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Collecting trio~=0.17 (from selenium->scholarly)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->scholarly)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->scholarly) (1.8.0)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->scholarly) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.11/dist-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (24.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->scholarly) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium->scholarly)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->scholarly) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->scholarly)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.2)\n",
            "Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: bibtexparser, free-proxy\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.3-py3-none-any.whl size=43550 sha256=80996e49762c8b670704cc4f42cea2c0257db71840935efc0985f1a9c1c11d89\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/fb/76/306387739cf9d53b1c39b0c8aadbbb17dc05f256756d8fd915\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for free-proxy: filename=free_proxy-1.1.3-py3-none-any.whl size=6096 sha256=000f59fb98185ce6d5fda5cc1c04b3d3317155df50c170d8503c733adcdcf3eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/f8/90/1e74c4166b7fbb213260a35e83fac3119f5e390c8bddda8a37\n",
            "Successfully built bibtexparser free-proxy\n",
            "Installing collected packages: wsproto, types-python-dateutil, outcome, fake-useragent, bibtexparser, trio, free-proxy, arrow, trio-websocket, sphinxcontrib-jquery, sphinx-rtd-theme, selenium, scholarly\n",
            "Successfully installed arrow-1.3.0 bibtexparser-1.4.3 fake-useragent-2.1.0 free-proxy-1.1.3 outcome-1.3.0.post0 scholarly-1.7.11 selenium-4.30.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 trio-0.29.0 trio-websocket-0.12.2 types-python-dateutil-2.9.0.20241206 wsproto-1.2.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.68.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=1e3617d4f733146c1517a8f0aedf7a037cc6dc7741e3465b1ff5cf864daef612\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=875e2bf87101112b43ce85111ef8edab3ab9b32241c7bad3a6b262dfb43bfd00\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n",
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.11/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pytrends) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->pytrends) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->pytrends) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->pytrends) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.17.0)\n",
            "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyautogen\n",
        "!pip install arxiv\n",
        "!pip install scholarly\n",
        "!pip install scikit-learn\n",
        "!pip install openai\n",
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install networkx\n",
        "!pip install PyPDF2\n",
        "!pip install sumy\n",
        "!pip install pytrends"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "4HCQUPHUPdSt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f82b096-11a7-4f5b-a44c-ff0fde2d19f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ag2[openai]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXDPFsrD-NK1",
        "outputId": "5e4627bd-5025-4091-c7e6-4593d6d2365c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ag2[openai]\n",
            "  Downloading ag2-0.8.3-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pyautogen==0.8.3 in /usr/local/lib/python3.11/dist-packages (from ag2[openai]) (0.8.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (4.9.0)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (0.0.8)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (5.6.3)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (7.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (2.10.6)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (1.1.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (2.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.3->ag2[openai]) (0.9.0)\n",
            "Requirement already satisfied: openai>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from pyautogen[openai]==0.8.3; extra == \"openai\"->ag2[openai]) (1.68.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.3->ag2[openai]) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.3->ag2[openai]) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.3->ag2[openai]) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen==0.8.3->ag2[openai]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen==0.8.3->ag2[openai]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->pyautogen==0.8.3->ag2[openai]) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.2->pyautogen[openai]==0.8.3; extra == \"openai\"->ag2[openai]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.2->pyautogen[openai]==0.8.3; extra == \"openai\"->ag2[openai]) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.2->pyautogen[openai]==0.8.3; extra == \"openai\"->ag2[openai]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.3->ag2[openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.3->ag2[openai]) (2.27.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.8.3->ag2[openai]) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.8.3->ag2[openai]) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->pyautogen==0.8.3->ag2[openai]) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->pyautogen==0.8.3->ag2[openai]) (3.4.1)\n",
            "Downloading ag2-0.8.3-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: ag2\n",
            "Successfully installed ag2-0.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import autogen\n",
        "from autogen import AssistantAgent, UserProxyAgent\n",
        "import arxiv\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import PyPDF2\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import re\n",
        "from collections import Counter\n",
        "import openai\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pytrends.request import TrendReq\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# LLM Configuration\n",
        "def get_llm_config():\n",
        "    return {\n",
        "        \"api_key\": userdata.get('OPENAI_API_KEY'),\n",
        "        \"model\": \"gpt-4-turbo\",\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 1000\n",
        "    }\n",
        "\n",
        "# Create Assistant Agent\n",
        "assistant_agent = AssistantAgent(\n",
        "    name=\"research_assistant\",\n",
        "    system_message=\"\"\"\n",
        "    You are an AI assistant specialized in research tasks, including paper recommendations, summarization, trend analysis, idea generation, and evaluate idea feasibility.\n",
        "    All responses must adhere to the following guidelines:\n",
        "\n",
        "    1) Do not fabricate information in the absence of sufficient evidence, and clearly indicate \"알 수 없습니다\" or \"잘 모르겠습니다\" if uncertain.\n",
        "    2) Verify possible information step-by-step before responding, marking ambiguous or unclear sources as \"확실하지 않음.\"\n",
        "    3) Base final responses only on verified information, keeping answers concise. If speculation is necessary, disclose it by stating \"추측입니다.\"\n",
        "    4) If the user's query is unclear or requires further information, first request additional context or details from the user.\n",
        "    5) Do not confidently assert unverified facts and provide evidence if necessary, including sources or references when available.\n",
        "    6) For every answer, specify supporting information with references or summarized related links and materials wherever possible.\n",
        "\n",
        "    # Steps\n",
        "    - **Paper Recommendations:**\n",
        "      1. Identify the specific field or topic area the user is interested in.\n",
        "      2. Search for recent and relevant papers within that field.\n",
        "      3. Filter and rank them based on relevance, publication date, and impact factor.\n",
        "      4. Provide a list of recommended papers with a brief description of each.\n",
        "\n",
        "    - **Summarization:**\n",
        "      1. Extract key points and findings from the provided paper or research material.\n",
        "      2. Highlight significant contributions and conclusions.\n",
        "      3. Write a concise summary that communicates the main insights.\n",
        "\n",
        "    - **Trend Analysis:**\n",
        "      1. Gather data or publications related to a specific research domain over time.\n",
        "      2. Identify patterns, common themes, emerging topics, and shifts in focus.\n",
        "      3. Present an analysis that explains these trends.\n",
        "\n",
        "    - **Idea Generation:**\n",
        "     1. Understand the user's area of interest and goals.\n",
        "     2. Brainstorm and gather inspiration from recent publications, trends, and gaps in the literature.\n",
        "     3. Present new research ideas or questions that can be pursued further.\n",
        "\n",
        "    # Output Format\n",
        "\n",
        "    - Responses should be in paragraph form, clearly structured and organized according to the task type.\n",
        "    - For lists, use bullet points or numbered lists where appropriate.\n",
        "    - Ensure any visualizations are described in detail with clear explanations for what they represent.\n",
        "    - Use a formal and informative tone suitable for academic or professional contexts.\n",
        "\n",
        "    # Notes\n",
        "\n",
        "    - Ensure recommendations are current and papers are from credible sources.\n",
        "    - For trend analysis, consider incorporating historical data and future predictions when possible.\n",
        "    - When generating ideas, ensure they are feasible and backed by current research to a reasonable extent.\"\"\",\n",
        "    llm_config=get_llm_config()\n",
        ")\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# Create User Proxy Agent\n",
        "def create_user_proxy():\n",
        "    return UserProxyAgent(\n",
        "        name=\"Admin\",\n",
        "        system_message=\"You are the user proxy handling requests and interacting with the AI assistant.\",\n",
        "        human_input_mode=\"ALWAYS\",\n",
        "        default_auto_reply=\"Reply 'TERMINATE' if the task is done.\"\n",
        "    )\n",
        "\n",
        "# Task functions\n",
        "def recommend_papers_tool(query: str, year: int = None, limit: int = 5) -> str:\n",
        "    try:\n",
        "        search_query = f\"{query}\"\n",
        "        if year:\n",
        "            search_query += f\" AND submittedDate:[{year}0101 TO {year}1231]\"\n",
        "\n",
        "        user = arxiv.Client()\n",
        "        search = arxiv.Search(query=search_query, max_results=limit, sort_by=arxiv.SortCriterion.Relevance)\n",
        "        papers = [\n",
        "            f\"Title: {result.title}\\nAuthors: {', '.join([author.name for author in result.authors])}\\nAbstract: {result.summary}\\nSubmitted Date: {result.updated.date().isoformat()}\\n\"\n",
        "            for result in user.results(search)\n",
        "        ]\n",
        "        return f\"Recommended papers for '{query}':\\n\\n\" + \"\\n\\n\".join(papers)\n",
        "    except Exception as e:\n",
        "\n",
        "        return f\"Error while recommending papers: {e}\"\n",
        "\n",
        "def summarize_pdf_tool(url: str) -> str:\n",
        "    try:\n",
        "        nltk.download('punkt_tab')\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        memory_file = BytesIO(response.content)\n",
        "        pdf_reader = PyPDF2.PdfReader(memory_file)\n",
        "        pdf_text = \"\".join(page.extract_text() for page in pdf_reader.pages)\n",
        "\n",
        "        if not pdf_text.strip():\n",
        "            return \"No text could be extracted from the PDF.\"\n",
        "\n",
        "        parser = PlaintextParser.from_string(pdf_text, Tokenizer(\"english\"))\n",
        "        summarizer = LexRankSummarizer()\n",
        "        total_sentences = len(parser.document.sentences)  # 전체 문장 개수 계산\n",
        "        summary_length = max(3, total_sentences // 5)  # 전체 문장의 20%를 요약 (최소 3문장)\n",
        "\n",
        "        summary = summarizer(parser.document, sentences_count=summary_length)\n",
        "        return \"PDF Summary: \" + \" \".join(str(sentence) for sentence in summary)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error summarizing PDF: {e}\"\n",
        "\n",
        "# 연구 트렌드 분석 툴\n",
        "\n",
        "def extract_research_trends(query: str, year: int = 2025, limit: int = 10) -> str:\n",
        "    papers = recommend_papers_tool(query, year, limit)\n",
        "    abstracts = [p.split(\"\\n\")[3] for p in papers.split(\"\\n\\n\") if \"Abstract:\" in p]\n",
        "\n",
        "    if not abstracts:\n",
        "        return \"No abstracts found. Unable to analyze trends.\"\n",
        "\n",
        "    # TF-IDF 적용\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=10)\n",
        "    tfidf_matrix = vectorizer.fit_transform(abstracts)\n",
        "    top_keywords = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # LLM을 활용해 트렌드 요약\n",
        "    prompt = f\"\"\"\n",
        "    최근 {query} 관련 연구에서 도출된 주요 키워드는 {', '.join(top_keywords)}입니다.\n",
        "    이 키워드를 기반으로 최근 연구 트렌드를 분석해 주세요.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# 연구 아이디어 생성 툴\n",
        "def generate_research_idea(query: str, year: int = 2025, limit: int = 20) -> str:\n",
        "    papers = recommend_papers_tool(query, year, limit)\n",
        "    abstracts = [p.split(\"\\n\")[3] for p in papers.split(\"\\n\\n\") if \"Abstract:\" in p]\n",
        "\n",
        "    if not abstracts:\n",
        "        return \"No abstracts found. Unable to generate research ideas.\"\n",
        "\n",
        "    # 연구 공백(한계점) 분석 → 새로운 연구 아이디어 도출\n",
        "    prompt = f\"\"\"\n",
        "    최근 {query} 관련 연구 논문을 참고하세요:\n",
        "    {abstracts[:3]}\n",
        "\n",
        "    1) 이 논문들에서 제시된 주요 연구 한계점(Gap)은 무엇인가요?\n",
        "    2) 이를 해결할 수 있는 새로운 연구 아이디어를 제안하세요.\n",
        "    3) 실현 가능성을 1~10점으로 평가하고, 이유를 설명하세요.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# 연구 아이디어 실현 가능성 평가 툴\n",
        "def evaluate_feasibility(research_idea: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "    Here is a research idea:\n",
        "    {research_idea}\n",
        "\n",
        "    다음 기준을 적용하여 실현 가능성을 평가하세요:\n",
        "    1) 기술 준비 수준(TRL) → 1~9단계\n",
        "    2) 데이터 가용성 (1: 매우 부족 ~ 10: 풍부)\n",
        "    3) 연구 수행 난이도 (1: 쉬움 ~ 10: 어려움)\n",
        "\n",
        "    각 항목별 점수를 부여하고, 총평을 작성하세요.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Register tools to the assistant\n",
        "assistant_agent.register_for_llm(name=\"recommend_papers\", description=\"Recommend research papers.\")(recommend_papers_tool)\n",
        "assistant_agent.register_for_llm(name=\"summarize_pdf\", description=\"Summarize plain text.\")(summarize_pdf_tool)\n",
        "assistant_agent.register_for_llm(name=\"extract_research_trends\", description=\"Analyze research trends from recent papers.\")(extract_research_trends)\n",
        "assistant_agent.register_for_llm(name=\"generate_research_idea\", description=\"Generate novel research ideas.\")(generate_research_idea)\n",
        "assistant_agent.register_for_llm(name=\"evaluate_feasibility\", description=\"Evaluate the feasibility of a research idea.\")(evaluate_feasibility)\n",
        "\n",
        "# Initialize User Proxy\n",
        "user_proxy = create_user_proxy()\n",
        "user_proxy.register_for_execution(name=\"recommend_papers\")(recommend_papers_tool)\n",
        "user_proxy.register_for_execution(name=\"summarize_pdf\")(summarize_pdf_tool)\n",
        "user_proxy.register_for_execution(name=\"extract_research_trends\")(extract_research_trends)\n",
        "user_proxy.register_for_execution(name=\"generate_research_idea\")(generate_research_idea)\n",
        "user_proxy.register_for_execution(name=\"evaluate_feasibility\")(evaluate_feasibility)\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to the interactive research assistant!\")\n",
        "    print(\"You can ask questions like:\")\n",
        "    print(\"- 추천: LLM에 관한 논문 3개 추천해줘, 2023년 이후.\")\n",
        "    print(\"- 요약: 연구 논문의 내용을 요약해줘.\")\n",
        "    print(\"- 연구 트렌드 분석: 특정 주제의 최신 연구 동향을 분석해줘.\")\n",
        "    print(\"- 연구 아이디어 생성: 새로운 연구 주제를 추천해줘.\")\n",
        "    print(\"- 연구 아이디어 실현 가능성 평가: 연구 아이디어의 현실성을 평가해줘.\")\n",
        "\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting the assistant. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            response = user_proxy.initiate_chat(assistant_agent, messages=[{\"role\": \"user\", \"content\": user_input}])\n",
        "            print(f\"Assistant: {response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "A0HhWcsbPgz6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46b44a97-5dec-47e0-8666-e166a0a2e1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the interactive research assistant!\n",
            "You can ask questions like:\n",
            "- 추천: LLM에 관한 논문 3개 추천해줘, 2023년 이후.\n",
            "- 요약: 연구 논문의 내용을 요약해줘.\n",
            "- 연구 트렌드 분석: 특정 주제의 최신 연구 동향을 분석해줘.\n",
            "- 연구 아이디어 생성: 새로운 연구 주제를 추천해줘.\n",
            "- 연구 아이디어 실현 가능성 평가: 연구 아이디어의 현실성을 평가해줘.\n",
            "You: 국가무형유산제도에 관한 논문 20개 추천\n",
            ">국가무형유산제도에 관한 논문 20개 추천\n",
            "Admin (to research_assistant):\n",
            "\n",
            "국가무형유산제도에 관한 논문 20개 추천\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "research_assistant (to Admin):\n",
            "\n",
            "***** Suggested tool call (call_ROtT9CCeIbodAe6X8rdSHArN): recommend_papers *****\n",
            "Arguments: \n",
            "{\"query\":\"국가무형유산제도\",\"limit\":20}\n",
            "*********************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as Admin. Provide feedback to research_assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 쿼리 영어로\n",
            "Admin (to research_assistant):\n",
            "\n",
            "쿼리 영어로\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "research_assistant (to Admin):\n",
            "\n",
            "***** Suggested tool call (call_bfA7X31axtnpYG13VdwPJDyG): recommend_papers *****\n",
            "Arguments: \n",
            "{\"query\":\"intangible cultural heritage systems\",\"limit\":20}\n",
            "*********************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as Admin. Provide feedback to research_assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 논문 제목만 영어로 유지하고 나머지는 한국어로 응답\n",
            "Admin (to research_assistant):\n",
            "\n",
            "논문 제목만 영어로 유지하고 나머지는 한국어로 응답\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "research_assistant (to Admin):\n",
            "\n",
            "***** Suggested tool call (call_kLMMqUXY5Z1fz0rpLzhwTkI1): recommend_papers *****\n",
            "Arguments: \n",
            "{\"query\":\"국가무형유산제도\",\"limit\":20}\n",
            "*********************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as Admin. Provide feedback to research_assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 쿼리는 영어로\n",
            "Admin (to research_assistant):\n",
            "\n",
            "쿼리는 영어로\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "research_assistant (to Admin):\n",
            "\n",
            "***** Suggested tool call (call_M0twJ3QuSJaLxtmAmGZN76MP): recommend_papers *****\n",
            "Arguments: \n",
            "{\"query\":\"intangible cultural heritage systems\",\"limit\":20}\n",
            "*********************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as Admin. Provide feedback to research_assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION recommend_papers...\n",
            "Call ID: call_M0twJ3QuSJaLxtmAmGZN76MP\n",
            "Input arguments: {'query': 'intangible cultural heritage systems', 'limit': 20}\n",
            "Admin (to research_assistant):\n",
            "\n",
            "***** Response from calling tool (call_M0twJ3QuSJaLxtmAmGZN76MP) *****\n",
            "Recommended papers for 'intangible cultural heritage systems':\n",
            "\n",
            "Title: Facilitating Daily Practice in Intangible Cultural Heritage through Virtual Reality: A Case Study of Traditional Chinese Flower Arrangement\n",
            "Authors: Yingna Wang, Qingqin Liu, Xiaoying Wei, Mingming Fan\n",
            "Abstract: The essence of intangible cultural heritage (ICH) lies in the living\n",
            "knowledge and skills passed down through generations. Daily practice plays a\n",
            "vital role in revitalizing ICH by fostering continuous learning and\n",
            "improvement. However, limited resources and accessibility pose significant\n",
            "challenges to sustaining such practice. Virtual reality (VR) has shown promise\n",
            "in supporting extensive skill training. Unlike technical skill training, ICH\n",
            "daily practice prioritizes cultivating a deeper understanding of cultural\n",
            "meanings and values. This study explores VR's potential in facilitating ICH\n",
            "daily practice through a case study of Traditional Chinese Flower Arrangement\n",
            "(TCFA). By investigating TCFA learners' challenges and expectations, we\n",
            "designed and evaluated FloraJing, a VR system enriched with cultural elements\n",
            "to support sustained TCFA practice. Findings reveal that FloraJing promotes\n",
            "progressive reflection, and continuous enhances technical improvement and\n",
            "cultural understanding. We further propose design implications for VR\n",
            "applications aimed at fostering ICH daily practice in both knowledge and\n",
            "skills.\n",
            "Submitted Date: 2025-03-12\n",
            "\n",
            "\n",
            "Title: Labeling of Cultural Heritage Collections on the Intersection of Visual Analytics and Digital Humanities\n",
            "Authors: Christofer Meinecke\n",
            "Abstract: Engaging in interdisciplinary projects on the intersection between\n",
            "visualization and humanities research can be a challenging endeavor. Challenges\n",
            "can be finding valuable outcomes for both domains, or how to apply\n",
            "state-of-the-art visual analytics methods like supervised machine learning\n",
            "algorithms. We discuss these challenges when working with cultural heritage\n",
            "data. Further, there is a gap in applying these methods to intangible heritage.\n",
            "To give a reflection on some interdisciplinary projects, we present three case\n",
            "studies focusing on the labeling of cultural heritage collections, the problems\n",
            "and challenges with the data, the participatory design process, and takeaways\n",
            "for the visualization scholars from these collaborations.\n",
            "Submitted Date: 2022-08-29\n",
            "\n",
            "\n",
            "Title: Entity Graph Extraction from Legal Acts -- a Prototype for a Use Case in Policy Design Analysis\n",
            "Authors: Anna Wróblewska, Bartosz Pieliński, Karolina Seweryn, Karol Saputa, Aleksandra Wichrowska, Sylwia Sysko-Romańczuk, Hanna Schreiber\n",
            "Abstract: This paper presents research on a prototype developed to serve the\n",
            "quantitative study of public policy design. This sub-discipline of political\n",
            "science focuses on identifying actors, relations between them, and tools at\n",
            "their disposal in health, environmental, economic, and other policies. Our\n",
            "system aims to automate the process of gathering legal documents, annotating\n",
            "them with Institutional Grammar, and using hypergraphs to analyse\n",
            "inter-relations between crucial entities. Our system is tested against the\n",
            "UNESCO Convention for the Safeguarding of the Intangible Cultural Heritage from\n",
            "2003, a legal document regulating essential aspects of international relations\n",
            "securing cultural heritage.\n",
            "Submitted Date: 2022-09-02\n",
            "\n",
            "\n",
            "Title: Automating the Analysis of Institutional Design in International Agreements\n",
            "Authors: Anna Wróblewska, Bartosz Pieliński, Karolina Seweryn, Sylwia Sysko-Romańczuk, Karol Saputa, Aleksandra Wichrowska, Hanna Schreiber\n",
            "Abstract: This paper explores the automatic knowledge extraction of formal\n",
            "institutional design - norms, rules, and actors - from international\n",
            "agreements. The focus was to analyze the relationship between the visibility\n",
            "and centrality of actors in the formal institutional design in regulating\n",
            "critical aspects of cultural heritage relations. The developed tool utilizes\n",
            "techniques such as collecting legal documents, annotating them with\n",
            "Institutional Grammar, and using graph analysis to explore the formal\n",
            "institutional design. The system was tested against the 2003 UNESCO Convention\n",
            "for the Safeguarding of the Intangible Cultural Heritage.\n",
            "Submitted Date: 2023-05-26\n",
            "\n",
            "\n",
            "Title: Evaluating Usability and Engagement of Large Language Models in Virtual Reality for Traditional Scottish Curling\n",
            "Authors: Ka Hei Carrie Lau, Efe Bozkir, Hong Gao, Enkelejda Kasneci\n",
            "Abstract: This paper explores the innovative application of Large Language Models\n",
            "(LLMs) in Virtual Reality (VR) environments to promote heritage education,\n",
            "focusing on traditional Scottish curling presented in the game ``Scottish\n",
            "Bonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with\n",
            "pre-defined scripted chatbots, evaluating key criteria such as usability, user\n",
            "engagement, and learning outcomes. The results show that LLM-based chatbots\n",
            "significantly improve interactivity and engagement, creating a more dynamic and\n",
            "immersive learning environment. This integration helps document and preserve\n",
            "cultural heritage and enhances dissemination processes, which are crucial for\n",
            "safeguarding intangible cultural heritage (ICH) amid environmental changes.\n",
            "Furthermore, the study highlights the potential of novel technologies in\n",
            "education to provide immersive experiences that foster a deeper appreciation of\n",
            "cultural heritage. These findings support the wider application of LLMs and VR\n",
            "in cultural education to address global challenges and promote sustainable\n",
            "practices to preserve and enhance cultural heritage.\n",
            "Submitted Date: 2024-09-25\n",
            "\n",
            "\n",
            "Title: Embodied Cognition Guides Virtual-Real Interaction Design to Help Yicheng Flower Drum Intangible Cultural Heritage Dissemination\n",
            "Authors: Yuhan Ma, Weiran Zhao, Xiaolin Zhang, Ze Gao\n",
            "Abstract: In order to make the non-heritage culture of Yicheng Flower Drum more\n",
            "relevant to the trend of the digital era and promote its dissemination and\n",
            "inheritance, the design and application of gesture recognition and virtual\n",
            "reality technologies guided by embodied cognition theory in the process of\n",
            "non-heritage culture dissemination is studied. At the same time, it will\n",
            "enhance the interaction between people and NRM culture, stimulate the\n",
            "audience's interest in understanding NRM and spreading NRM, and create\n",
            "awareness of preserving NRM culture. Using embodied cognition as a theoretical\n",
            "guide, expanding the unidirectional communication mode through human-computer\n",
            "interaction close to natural behavior and cooperating with multisensory\n",
            "information reception channels, so as to construct an embodied and immersive\n",
            "interactive atmosphere for the participants and enable them to naturally form\n",
            "the cognition and understanding of the traditional culture in the process of\n",
            "interaction. The dissemination of the non-heritage culture Yicheng Flower Drum\n",
            "can take the theory of embodied cognition as an entry point, and through the\n",
            "virtual and real scenes of Yicheng Flower Drum and the immersive experience, we\n",
            "can empower the interaction design of non-heritage culture dissemination of the\n",
            "virtual and real, and provide a new method for the research of digital design\n",
            "of non-heritage culture.\n",
            "Submitted Date: 2023-10-07\n",
            "\n",
            "\n",
            "Title: CalliSense: An Interactive Educational Tool for Process-based Learning in Chinese Calligraphy\n",
            "Authors: Xinya Gong, Wenhui Tao, Yuxin Ma\n",
            "Abstract: Process-based learning is crucial for the transmission of intangible cultural\n",
            "heritage, especially in complex arts like Chinese calligraphy, where mastering\n",
            "techniques cannot be achieved by merely observing the final work. To explore\n",
            "the challenges faced in calligraphy heritage transmission, we conducted\n",
            "semi-structured interviews (N=8) as a formative study. Our findings indicate\n",
            "that the lack of calligraphy instructors and tools makes it difficult for\n",
            "students to master brush techniques, and teachers struggle to convey the\n",
            "intricate details and rhythm of brushwork. To address this, we collaborated\n",
            "with calligraphy instructors to develop an educational tool that integrates\n",
            "writing process capture and visualization, showcasing the writing rhythm, hand\n",
            "force, and brush posture. Through empirical studies conducted in multiple\n",
            "teaching workshops, we evaluated the system's effectiveness with teachers (N=4)\n",
            "and students (N=12). The results show that the tool significantly enhances\n",
            "teaching efficiency and aids learners in better understanding brush techniques.\n",
            "Submitted Date: 2025-02-21\n",
            "\n",
            "\n",
            "Title: The Heritage Digital Twin: a bicycle made for two. The integration of digital methodologies into cultural heritage research\n",
            "Authors: Franco Niccolucci, Béatrice Markhoff, Maria Theodoridou, Achille Felicetti, Sorin Hermon\n",
            "Abstract: The paper concerns the definition of a novel ontology for cultural heritage\n",
            "based on the concept of digital twin. The ontology, called Heritage Digital\n",
            "Twin ontology, is a compatible extension of the well-known CIDOC CRM ISO\n",
            "standard for cultural heritage documentation and incorporates all the different\n",
            "documentation systems presently in use for cultural heritage documentation. In\n",
            "the authors' view, it supports documentation interoperability at a higher level\n",
            "than the ones currently in use and enables effective cooperation among\n",
            "different users.\n",
            "Submitted Date: 2023-02-03\n",
            "\n",
            "\n",
            "Title: Expected vibroacoustic behaviour of Greek Doric-style temples and its relation with geometrical physics design as part of the intangible cultural heritage\n",
            "Authors: Fabrizio Barone, Marco Casazza\n",
            "Abstract: This study proposes a new approach to the interpretation of Greek Doric-style\n",
            "temples, based on the integration of its tangible and intangible dimensions as\n",
            "a cultural heritage asset. Rooted on the Greek concept of techne, the work\n",
            "considers a unifying design principle, integrating both structural and\n",
            "functional aspects within the architectural style. A multidisciplinary\n",
            "perspective was adopted, combining archaeological, documentary, and\n",
            "metrological analysis of 41 Doric temples from the 6th to the 4th century BC,\n",
            "located in Greece and Magna Graecia. Starting from the evidence of a\n",
            "statistical correlation among key geometric parameters, these quantitative data\n",
            "are re-interpreted through a geometrical physics vibroacoustic model. The\n",
            "results demonstrate that structural elements act as acoustic attenuators,\n",
            "minimizing environmental forces (particularly wind) on the temple cell's walls.\n",
            "The study also suggests that slight deviations from the classic East-West\n",
            "orientation were adopted to reduce the acoustic coupling with prevailing local\n",
            "winds. The Archaeological Park of Paestum (Salerno, Italy) provides significant\n",
            "evidence for this hypothesis, as its temples, despite their different\n",
            "construction periods, share a consistent orientation, distinct from the city's\n",
            "street grid. These findings contribute to a deeper understanding of Greek\n",
            "know-how, being a part of the intangible dimension of cultural heritage and\n",
            "traditional ecological knowledge related to the architectural design in\n",
            "relation to the environmental factors.\n",
            "Submitted Date: 2025-03-10\n",
            "\n",
            "\n",
            "Title: Revival of the Silk Road using the applications of AR/VR and its role on cultural tourism\n",
            "Authors: Sahar Zandi\n",
            "Abstract: This research project seeks to investigate the incorporation of augmented\n",
            "reality (AR) and virtual reality (VR) technology with human-computer\n",
            "interaction (HCI) in order to revitalize the Silk Road - specifically in\n",
            "Kermanshah, Iran - and its effect on cultural tourism. Kermanshah has\n",
            "underexplored the rich historical significance of the Silk Road, despite the\n",
            "presence of 24 UNESCO World Heritage sites. From the 2nd century BCE to the\n",
            "18th century CE, the Silk Road was a vital trade route connecting the West and\n",
            "the East and had enormous cultural, economic, religious, and political effects.\n",
            "The purpose of this study is to examine the application of AR/VR technologies\n",
            "in HCI for the preservation, interpretation, and promotion of the Silk Road's\n",
            "tangible and intangible cultural heritage in Kermanshah, as well as their\n",
            "impact on cultural tourism development. The study also investigates how these\n",
            "innovative technologies can enhance visitors' experiences through immersive and\n",
            "interactive approaches, promote sustainable tourism practices, and contribute\n",
            "to the region's broader socioeconomic benefits. The research will analyze the\n",
            "challenges and opportunities of implementing AR/VR technology in HCI within the\n",
            "context of cultural heritage and tourism in Kermanshah and the Silk Road region\n",
            "more broadly. By combining HCI, AR/VR, and cultural tourism, this research\n",
            "seeks to provide valuable insights into the development of user-centered,\n",
            "immersive experiences that promote a deeper understanding and appreciation of\n",
            "the Silk Road's distinctive cultural heritage.\n",
            "Submitted Date: 2023-04-13\n",
            "\n",
            "\n",
            "Title: Cultural gems linked open data: Mapping culture and intangible heritage in European cities\n",
            "Authors: Sergio Consoli, Valentina Alberti, Cinzia Cocco, Francesco Panella, Valentina Montalto\n",
            "Abstract: The recovery and resilience of the cultural and creative sectors after the\n",
            "COVID-19 pandemic is a current topic with priority for the European Commission.\n",
            "Cultural gems is a crowdsourced web platform managed by the Joint Research\n",
            "Centre of the European Commission aimed at creating community-led maps as well\n",
            "as a common repository for cultural and creative places across European cities\n",
            "and towns. More than 130,000 physical locations and online cultural activities\n",
            "in more than 300 European cities and towns are currently tracked by the\n",
            "application. The main objective of Cultural gems consists in raising a holistic\n",
            "vision of European culture, reinforcing a sense of belonging to a common\n",
            "European cultural space. This data article describes the ontology developed for\n",
            "Cultural gems, adopted to represent the domain of knowledge of the application\n",
            "by means of FAIR (Findable, Accessible, Interoperable, Reusable) principles and\n",
            "following the paradigms of Linked Open Data (LOD). We provide an overview of\n",
            "this dataset, and describe the ontology model, along with the services used to\n",
            "access and consume the data.\n",
            "Submitted Date: 2024-02-12\n",
            "\n",
            "\n",
            "Title: Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO's Summary Records\n",
            "Authors: Joanna Wojciechowska, Mateusz Sypniewski, Maria Śmigielska, Igor Kamiński, Emilia Wiśnios, Hanna Schreiber, Bartosz Pieliński\n",
            "Abstract: Cultural heritage is an arena of international relations that interests all\n",
            "states worldwide. The inscription process on the UNESCO World Heritage List and\n",
            "the UNESCO Representative List of the Intangible Cultural Heritage of Humanity\n",
            "often leads to tensions and conflicts among states. This research addresses\n",
            "these challenges by developing automatic tools that provide valuable insights\n",
            "into the decision-making processes regarding inscriptions to the two lists\n",
            "mentioned above. We propose innovative topic modelling and tension detection\n",
            "methods based on UNESCO's summary records. Our analysis achieved a commendable\n",
            "accuracy rate of 72% in identifying tensions. Furthermore, we have developed an\n",
            "application tailored for diplomats, lawyers, political scientists, and\n",
            "international relations researchers that facilitates the efficient search of\n",
            "paragraphs from selected documents and statements from specific speakers about\n",
            "chosen topics. This application is a valuable resource for enhancing the\n",
            "understanding of complex decision-making dynamics within international heritage\n",
            "inscription procedures.\n",
            "Submitted Date: 2023-08-01\n",
            "\n",
            "\n",
            "Title: \"It Is Hard to Remove from My Eye\": Design Makeup Residue Visualization System for Chinese Traditional Opera (Xiqu) Performers\n",
            "Authors: Zeyu Xiong, Shihan Fu, Yanying Zhu, Chenqing Zhu, Xiaojuan Ma, Mingming Fan\n",
            "Abstract: Chinese traditional opera (Xiqu) performers often experience skin problems\n",
            "due to the long-term use of heavy-metal-laden face paints. To explore the\n",
            "current skincare challenges encountered by Xiqu performers, we conducted an\n",
            "online survey (N=136) and semi-structured interviews (N=15) as a formative\n",
            "study. We found that incomplete makeup removal is the leading cause of\n",
            "human-induced skin problems, especially the difficulty in removing eye makeup.\n",
            "Therefore, we proposed EyeVis, a prototype that can visualize the residual eye\n",
            "makeup and record the time make-up was worn by Xiqu performers. We conducted a\n",
            "7-day deployment study (N=12) to evaluate EyeVis. Results indicate that EyeVis\n",
            "helps to increase Xiqu performers' awareness about removing makeup, as well as\n",
            "boosting their confidence and security in skincare. Overall, this work also\n",
            "provides implications for studying the work of people who wear makeup on a\n",
            "daily basis, and helps to promote and preserve the intangible cultural heritage\n",
            "of practitioners.\n",
            "Submitted Date: 2024-02-24\n",
            "\n",
            "\n",
            "Title: Chinese Traditional Poetry Generating System Based on Deep Learning\n",
            "Authors: Chenlei Bao, Lican Huang\n",
            "Abstract: Chinese traditional poetry is an important intangible cultural heritage of\n",
            "China and an artistic carrier of thought, culture, spirit and emotion. However,\n",
            "due to the strict rules of ancient poetry, it is very difficult to write poetry\n",
            "by machine. This paper proposes an automatic generation method of Chinese\n",
            "traditional poetry based on deep learning technology, which extracts keywords\n",
            "from each poem and matches them with the previous text to make the poem conform\n",
            "to the theme, and when a user inputs a paragraph of text, the machine obtains\n",
            "the theme and generates poem sentence by sentence. Using the classic word2vec\n",
            "model as the preprocessing model, the Chinese characters which are not\n",
            "understood by the computer are transformed into matrix for processing.\n",
            "Bi-directional Long Short-Term Memory is used as the neural network model to\n",
            "generate Chinese characters one by one and make the meaning of Chinese\n",
            "characters as accurate as possible. At the same time, TF-IDF and TextRank are\n",
            "used to extract keywords. Using the attention mechanism based encoding-decoding\n",
            "model, we can solve practical problems by transforming the model, and\n",
            "strengthen the important information of long-distance information, so as to\n",
            "grasp the key points without losing important information. In the aspect of\n",
            "emotion judgment, Long Short-Term Memory network is used. The final result\n",
            "shows that it can get good poetry outputs according to the user input text.\n",
            "Submitted Date: 2021-10-24\n",
            "\n",
            "\n",
            "Title: Promoting the Culture of Qinhuai River Lantern Shadow Puppetry with a Digital Archive and Immersive Experience\n",
            "Authors: Yuanfang Liu, Rua Mae Williams, Guanghong Xie, Yu Wang, Wenrui Zuo\n",
            "Abstract: As an intangible cultural heritage, Chinese shadow puppetry is facing\n",
            "challenges in terms of its appeal and comprehension, especially among audiences\n",
            "from different cultural backgrounds. Additionally, the fragile materials of the\n",
            "puppets and obstacles to preservation pose further challenges. This study\n",
            "creates a digital archive of the Qinhuai River Lantern Festival shadow\n",
            "puppetry, utilizing digital technology to recreate scenes depicted in\n",
            "traditional Chinese poetry and painting. Moreover, this study employs a\n",
            "mixed-method approach, combining qualitative and quantitative methods, to\n",
            "evaluate the acceptance and audience experience of immersive shadow puppetry.\n",
            "An in-depth exploration was conducted from sensory, emotional, cultural\n",
            "dimensions and research hypotheses were tested using structural equation\n",
            "modeling and other methods. The results indicate that enhancing ease of use and\n",
            "cultural experience can improve audience appeal and comprehension, while\n",
            "enhancing emotional experience can increase audience participation intention.\n",
            "Our research holds profound significance for the preservation and transmission\n",
            "of shadow puppetry.\n",
            "Submitted Date: 2024-10-16\n",
            "\n",
            "\n",
            "Title: Personalized Generative AI in VR for Enhanced Engagement: Eye-Tracking Insights into Cultural Heritage Learning through Neapolitan Pizza Making\n",
            "Authors: Ka Hei Carrie Lau, Sema Sen, Philipp Stark, Efe Bozkir, Enkelejda Kasneci\n",
            "Abstract: Virtual Reality (VR) and Generative Artificial Intelligence (Gen-AI) are\n",
            "transforming personalized learning, particularly in intangible cultural\n",
            "heritage (ICH) education. However, designing immersive experiences that enhance\n",
            "engagement without overwhelming learners presents a challenge. This study\n",
            "examines the impact of personalized AI narration on user engagement and\n",
            "attention in a VR environment through eye-tracking metrics. In a controlled\n",
            "experiment with 54 participants, we explored three levels of personalization\n",
            "(high, moderate, none) in a Neapolitan pizza-making task, measuring attention\n",
            "and cognitive load through fixation duration, saccade duration, and pupil\n",
            "diameter. Results indicate that high personalization increased engagement by\n",
            "64.1% over no personalization (p < 0.001). Furthermore, regression analysis\n",
            "reveals specific eye-tracking metrics significantly predict gameplay duration,\n",
            "underscoring eye-tracking's potential to capture real-time engagement. These\n",
            "findings support the use of eye-tracking to inform the development of adaptive\n",
            "VR learning experiences. Future work may integrate subjective assessments to\n",
            "better understand users' underlying motivations.\n",
            "Submitted Date: 2024-11-27\n",
            "\n",
            "\n",
            "Title: Computational lexical analysis of Flamenco genres\n",
            "Authors: Pablo Rosillo-Rodes, Maxi San Miguel, David Sanchez\n",
            "Abstract: Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of\n",
            "Humanity, is a profound expression of cultural identity rooted in Andalusia,\n",
            "Spain. However, there is a lack of quantitative studies that help identify\n",
            "characteristic patterns in this long-lived music tradition. In this work, we\n",
            "present a computational analysis of Flamenco lyrics, employing natural language\n",
            "processing and machine learning to categorize over 2000 lyrics into their\n",
            "respective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial\n",
            "Naive Bayes classifier, we find that lexical variation across styles enables to\n",
            "accurately identify distinct $\\textit{palos}$. More importantly, from an\n",
            "automatic method of word usage, we obtain the semantic fields that characterize\n",
            "each style. Further, applying a metric that quantifies the inter-genre distance\n",
            "we perform a network analysis that sheds light on the relationship between\n",
            "Flamenco styles. Remarkably, our results suggest historical connections and\n",
            "$\\textit{palo}$ evolutions. Overall, our work illuminates the intricate\n",
            "relationships and cultural significance embedded within Flamenco lyrics,\n",
            "complementing previous qualitative discussions with quantitative analyses and\n",
            "sparking new discussions on the origin and development of traditional music\n",
            "genres.\n",
            "Submitted Date: 2024-05-09\n",
            "\n",
            "\n",
            "Title: Hidden costs of La Mancha's production model and drivers of change\n",
            "Authors: Máximo Florín, Rafael U. Gosálvez\n",
            "Abstract: The territory of La Mancha, its rural areas, and its landscapes suffer a kind\n",
            "of atherosclerosis (\"the silent killer\") because of the increase in artificial\n",
            "surfaces, the fragmentation of the countryside by various infrastructures, the\n",
            "abandonment of small and medium-sized farms and the loss of agricultural,\n",
            "material, and intangible heritage. At the same time, agricultural\n",
            "industrialization hides, behind a supposed productive efficiency, the\n",
            "deterioration of the quantitative and qualitative ecological status of surface\n",
            "and groundwater bodies, and causes air pollution, greenhouse gas emissions,\n",
            "loss of soil fertility, drainage and plowing of wetlands, forgetfulness of the\n",
            "ancestral environmental heritage, of the emergence of uses and customs of\n",
            "collective self-government and reduction of the adaptive capacity of\n",
            "traditional agroecosystems. This work aims, firstly, to shed light on the true\n",
            "costs of the main causes of environmental degradation in the territory of La\n",
            "Mancha, while deteriorating relations between rural and urban areas and\n",
            "determining the loss of territorial identity of La Mancha. the population. In\n",
            "addition, drivers of change toward a more sustainable social, economic,\n",
            "hydrological, environmental, and cultural production model are identified.\n",
            "Submitted Date: 2022-12-27\n",
            "\n",
            "\n",
            "Title: Ontology Engineering to Model the European Cultural Heritage: The Case of Cultural Gems\n",
            "Authors: Valentina Alberti, Cinzia Cocco, Sergio Consoli, Valentina Montalto, Francesco Panella\n",
            "Abstract: Cultural gems is a web application conceived by the European Commission's\n",
            "Joint Research Centre (DG JRC), which aims at engaging people and organisations\n",
            "across Europe to create a unique repository of cultural and creative places.\n",
            "The main goal is to provide a vision of European culture in order to strengthen\n",
            "a sense of identity within a single European cultural realm. Cultural gems maps\n",
            "more than 130,000 physical places in over 300 European cities and towns, and\n",
            "since 2020 it also lists online cultural initiatives. The new release aims,\n",
            "among other, to increase the interoperability of the application. At this\n",
            "purpose, we provide an overview on the current development of an ontology for\n",
            "Cultural gems used to map cultural heritage in European cities by using Linked\n",
            "Open Data (LOD) standards, and making the data FAIR, that is Findable,\n",
            "Accessible, Interoperable, and Reusable. We provide an overview of the\n",
            "methodology, presenting the structure of the ontology, and the services and\n",
            "tools we are currently building on top.\n",
            "Submitted Date: 2024-02-12\n",
            "\n",
            "\n",
            "Title: Preserving Tangible and Intangible Cultural Heritage: the Cases of Volterra and Atari\n",
            "Authors: Maciej Grzeszczuk, Kinga Skorupska, Paweł Grabarczyk, Władysław Fuchs, Paul F. Aubin, Mark E. Dietrick, Barbara Karpowicz, Rafał Masłyk, Pavlo Zinevych, Wiktor Stawski, Stanisław Knapiński, Wiesław Kopeć\n",
            "Abstract: At first glance, the ruins of the Roman Theatre in the Italian town of\n",
            "Volterra have little in common with cassette tapes containing Atari games. One\n",
            "is certainly considered an important historical landmark, while the consensus\n",
            "on the importance of the other is partial at best. Still, both are remnants of\n",
            "times vastly different from the present and are at risk of oblivion. Unearthed\n",
            "architectural structures are exposed to the elements just as the deteriorating\n",
            "signals stored on magnetic tapes. However, the rate of deterioration is much\n",
            "faster with the magnetic media, as their life expectancy is counted in decades,\n",
            "whereas the Roman Theater, which is already in ruin, measures its lifespan in\n",
            "centuries. Hence, both would benefit from some form of digital preservation and\n",
            "reconstruction. In this panel, we discuss how to sustainably preserve tangible\n",
            "and intangible cultural artifacts for future generations.\n",
            "Submitted Date: 2024-03-05\n",
            "\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "research_assistant (to Admin):\n",
            "\n",
            "다음은 '국가무형유산제도'에 관한 논문 20개의 추천 목록입니다:\n",
            "\n",
            "1. **Facilitating Daily Practice in Intangible Cultural Heritage through Virtual Reality: A Case Study of Traditional Chinese Flower Arrangement** - Yingna Wang 등\n",
            "   - 이 논문은 가상현실을 활용하여 중국 전통 꽃꽂이의 일상적인 실습을 지원하는 시스템을 탐구합니다.\n",
            "\n",
            "2. **Labeling of Cultural Heritage Collections on the Intersection of Visual Analytics and Digital Humanities** - Christofer Meinecke\n",
            "   - 문화 유산 데이터를 처리하고 시각화하는 데 있어서의 도전과 기회에 대해 논의합니다.\n",
            "\n",
            "3. **Entity Graph Extraction from Legal Acts -- a Prototype for a Use Case in Policy Design Analysis** - Anna Wróblewska 등\n",
            "   - 법적 문서에서 중요한 엔티티 간의 관계를 분석하는 시스템을 개발하고 유네스코 국가무형유산 협약을 대상으로 적용한 연구입니다.\n",
            "\n",
            "4. **Automating the Analysis of Institutional Design in International Agreements** - Anna Wróblewska 등\n",
            "   - 국제 협약에서 기관의 설계를 자동으로 분석하는 도구를 개발하고 무형 문화 유산의 규제에 초점을 맞춘 연구입니다.\n",
            "\n",
            "5. **Evaluating Usability and Engagement of Large Language Models in Virtual Reality for Traditional Scottish Curling** - Ka Hei Carrie Lau 등\n",
            "   - 대형 언어 모델을 활용한 가상 현실 환경에서의 사용성과 참여도를 평가합니다.\n",
            "\n",
            "6. **Embodied Cognition Guides Virtual-Real Interaction Design to Help Yicheng Flower Drum Intangible Cultural Heritage Dissemination** - Yuhan Ma 등\n",
            "   - 신체 인지 이론을 기반으로 하여 가상과 현실의 상호작용을 통해 이청 플라워 드럼의 전파를 돕는 연구입니다.\n",
            "\n",
            "7. **CalliSense: An Interactive Educational Tool for Process-based Learning in Chinese Calligraphy** - Xinya Gong 등\n",
            "   - 중국 서예의 전수를 위한 상호작용 교육 도구를 개발하고 효과를 평가한 논문입니다.\n",
            "\n",
            "8. **The Heritage Digital Twin: a bicycle made for two. The integration of digital methodologies into cultural heritage research** - Franco Niccolucci 등\n",
            "   - 문화 유산 연구에 디지털 방법론을 통합하는 새로운 온톨로지를 제안합니다.\n",
            "\n",
            "9. **Expected vibroacoustic behaviour of Greek Doric-style temples and its relation with geometrical physics design as part of the intangible cultural heritage** - Fabrizio Barone 등\n",
            "   - 그리스 도릭식 사원의 진동 음향 행동과 그 구조적 설계 원리를 연구합니다.\n",
            "\n",
            "10. **Revival of the Silk Road using the applications of AR/VR and its role on cultural tourism** - Sahar Zandi\n",
            "    - 증강 현실과 가상 현실을 활용하여 실크로드의 부활과 문화 관광에 미치는 영향을 탐구합니다.\n",
            "\n",
            "11. **Cultural gems linked open data: Mapping culture and intangible heritage in European cities** - Sergio Consoli 등\n",
            "    - 유럽 도시의 문화와 무형 문화 유산을 매핑하는 웹 플랫폼에 대해 설명합니다.\n",
            "\n",
            "12. **Deep Dive into the Language of International Relations: NLP-based Analysis of UNESCO's Summary Records** - Joanna Wojciechowska 등\n",
            "    - 유네스코의 요약 기록을 분석하여 국제 관계의 언어를 탐구하는 연구입니다\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4b5f72ba1870>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_proxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massistant_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Assistant: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36minitiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m                 \u001b[0mmsg2send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_init_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg2send\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         summary = self._summarize_chat(\n\u001b[1;32m   1510\u001b[0m             \u001b[0msummary_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreply_at_receive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2506\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trigger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m                 \u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogging_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m                     log_event(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mcheck_termination_and_human_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   2203\u001b[0m         \u001b[0msender_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the sender\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msender\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_input_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ALWAYS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2205\u001b[0;31m             reply = self.get_human_input(\n\u001b[0m\u001b[1;32m   2206\u001b[0m                 \u001b[0;34mf\"Replying as {self.name}. Provide feedback to {sender_name}. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mget_human_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   2636\u001b[0m         \u001b[0miostream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIOStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2638\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miostream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_human_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2640\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen/io/console.py\u001b[0m in \u001b[0;36minput\u001b[0;34m(self, prompt, password)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"Password: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}